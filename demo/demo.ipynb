{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "def redditScraper(x):\n",
    "    reddit = praw.Reddit(user_agent='CommentScraper (by /u/ConnectBad)',\n",
    "                         client_id='dHUYT7-TgK2aKA', client_secret=\"Sg2ZreLSzOYqURbJK4QA5Ysh7tM\",\n",
    "                         username='ConnectBad', password='passwordfor180')\n",
    "\n",
    "    subreddit = reddit.subreddit(x)\n",
    "\n",
    "    myFile = open('arsenal_reddit_comments.csv', 'w') \n",
    "    writer = csv.writer(myFile, dialect='excel')\n",
    "    writer.writerow(['First 2000 Comments for each post on Hot Page\\'s top 40'])\n",
    "\n",
    "    counter = 0;\n",
    "    for submission in subreddit.hot(limit=20):\n",
    "        submission.comments.replace_more(limit=200)\n",
    "        for comment in submission.comments.list():\n",
    "            writer.writerow([comment.body.encode('utf-8')])\n",
    "            counter = counter + 1\n",
    "            if counter >= 2000:\n",
    "                break\n",
    "    myFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "try:\n",
    "    from urllib.request import urlopen, Request\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen, Request\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try:\n",
    "            response = urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            time.sleep(5)\n",
    "\n",
    "            #print(\"Error for URL {}: {}\".format(url, datetime.datetime.now()))\n",
    "            #print(\"Retrying.\")\n",
    "            return 0\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "\n",
    "# Needed to write tricky unicode correctly to csv\n",
    "def unicode_decode(text):\n",
    "    try:\n",
    "        return text.encode('utf-8').decode()\n",
    "    except UnicodeDecodeError:\n",
    "        return text.encode('utf-8')\n",
    "\n",
    "\n",
    "def getFacebookPageFeedUrl(base_url):\n",
    "\n",
    "    # Construct the URL string; see http://stackoverflow.com/a/37239851 for\n",
    "    # Reactions parameters\n",
    "    fields = \"&fields=message,link,created_time,type,name,id,\" + \\\n",
    "        \"comments.limit(0).summary(true),shares,reactions\" + \\\n",
    "        \".limit(0).summary(true)\"\n",
    "\n",
    "    return base_url + fields\n",
    "\n",
    "\n",
    "def getFacebookCommentFeedUrl(base_url):\n",
    "\n",
    "    # Construct the URL string\n",
    "    fields = \"&fields=id,message\"\n",
    "    url = base_url + fields\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def getReactionsForStatuses(base_url):\n",
    "\n",
    "    reaction_types = ['like', 'love', 'wow', 'haha', 'sad', 'angry']\n",
    "    reactions_dict = {}   # dict of {status_id: tuple<6>}\n",
    "\n",
    "    for reaction_type in reaction_types:\n",
    "        fields = \"&fields=reactions.type({}).limit(0).summary(total_count)\".format(\n",
    "            reaction_type.upper())\n",
    "\n",
    "        url = base_url + fields\n",
    "\n",
    "        reqResult = request_until_succeed(url)\n",
    "\n",
    "        if(reqResult != 0):\n",
    "            data = json.loads(reqResult)['data']\n",
    "\n",
    "        data_processed = set()  # set() removes rare duplicates in statuses\n",
    "        for status in data:\n",
    "            id = status['id']\n",
    "            count = status['reactions']['summary']['total_count']\n",
    "            data_processed.add((id, count))\n",
    "\n",
    "        for id, count in data_processed:\n",
    "            if id in reactions_dict:\n",
    "                reactions_dict[id] = reactions_dict[id] + (count,)\n",
    "            else:\n",
    "                reactions_dict[id] = (count,)\n",
    "\n",
    "    return reactions_dict\n",
    "\n",
    "\n",
    "def processFacebookPageFeedStatus(status):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    status_id = status['id']\n",
    "    status_type = status['type']\n",
    "\n",
    "    status_message = '' if 'message' not in status else \\\n",
    "        unicode_decode(status['message'])\n",
    "    link_name = '' if 'name' not in status else \\\n",
    "        unicode_decode(status['name'])\n",
    "    status_link = '' if 'link' not in status else \\\n",
    "        unicode_decode(status['link'])\n",
    "\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "\n",
    "    status_published = datetime.datetime.strptime(\n",
    "        status['created_time'], '%Y-%m-%dT%H:%M:%S+0000')\n",
    "    status_published = status_published + \\\n",
    "        datetime.timedelta(hours=-5)  # EST\n",
    "    status_published = status_published.strftime(\n",
    "        '%Y-%m-%d %H:%M:%S')  # best time format for spreadsheet programs\n",
    "\n",
    "    # Nested items require chaining dictionary keys.\n",
    "\n",
    "    num_reactions = 0 if 'reactions' not in status else \\\n",
    "        status['reactions']['summary']['total_count']\n",
    "    num_comments = 0 if 'comments' not in status else \\\n",
    "        status['comments']['summary']['total_count']\n",
    "    num_shares = 0 if 'shares' not in status else status['shares']['count']\n",
    "\n",
    "    return (status_id, status_message, link_name, status_type, status_link,\n",
    "            status_published, num_reactions, num_comments, num_shares)\n",
    "\n",
    "\n",
    "def scrapeFacebookPageFeedStatus(page_id, access_token, since_date, until_date):\n",
    "    with open('{}_facebook_statuses.csv'.format(page_id), 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"status_id\", \"status_message\", \"link_name\", \"status_type\",\n",
    "                    \"status_link\", \"status_published\", \"num_reactions\",\n",
    "                    \"num_comments\", \"num_shares\", \"num_likes\", \"num_loves\",\n",
    "                    \"num_wows\", \"num_hahas\", \"num_sads\", \"num_angrys\",\n",
    "                    \"num_special\"])\n",
    "\n",
    "        has_next_page = True\n",
    "        num_processed = 0\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "        after = ''\n",
    "        base = \"https://graph.facebook.com/v2.12\"\n",
    "        node = \"/{}/posts\".format(page_id)\n",
    "        parameters = \"/?limit={}&access_token={}\".format(100, access_token)\n",
    "        since = \"&since={}\".format(since_date) if since_date \\\n",
    "            is not '' else ''\n",
    "        until = \"&until={}\".format(until_date) if until_date \\\n",
    "            is not '' else ''\n",
    "\n",
    "        print(\"Scraping {} Facebook Page: {}\\n\".format(page_id, scrape_starttime))\n",
    "\n",
    "        while has_next_page:\n",
    "            after = '' if after is '' else \"&after={}\".format(after)\n",
    "            base_url = base + node + parameters + after + since + until\n",
    "\n",
    "            url = getFacebookPageFeedUrl(base_url)\n",
    "\n",
    "            reqResult = request_until_succeed(url)\n",
    "            \n",
    "            if(reqResult != 0):\n",
    "                statuses = json.loads(reqResult)\n",
    "\n",
    "            reactions = getReactionsForStatuses(base_url)\n",
    "\n",
    "            for status in statuses['data']:\n",
    "\n",
    "                # Ensure it is a status with the expected metadata\n",
    "                if 'reactions' in status:\n",
    "                    status_data = processFacebookPageFeedStatus(status)\n",
    "                    reactions_data = reactions[status_data[0]]\n",
    "\n",
    "                    # calculate thankful/pride through algebra\n",
    "                    num_special = status_data[6] - sum(reactions_data)\n",
    "                    w.writerow(status_data + reactions_data + (num_special,))\n",
    "\n",
    "                num_processed += 1\n",
    "                if num_processed % 100 == 0:\n",
    "                    print(\"{} Statuses Processed: {}\".format\n",
    "                          (num_processed, datetime.datetime.now()))\n",
    "\n",
    "            # if there is no next page, we're done.\n",
    "            if 'paging' in statuses:\n",
    "                after = statuses['paging']['cursors']['after']\n",
    "            else:\n",
    "                has_next_page = False\n",
    "\n",
    "        print(\"\\nDone!\\n{} Statuses Processed in {}\".format(\n",
    "              num_processed, datetime.datetime.now() - scrape_starttime))\n",
    "\n",
    "\n",
    "def processFacebookComment(comment, status_id, parent_id=''):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    #comment_id = comment['id']\n",
    "    comment_message = '' if 'message' not in comment or comment['message'] \\\n",
    "        is '' else unicode_decode(comment['message'])\n",
    "\n",
    "    # Return a tuple of all processed data\n",
    "\n",
    "    #return (comment_id, status_id, parent_id, comment_message)\n",
    "    return(comment_message, '')\n",
    "\n",
    "\n",
    "def scrapeFacebookPageFeedComments(page_id, access_token, file_id):\n",
    "    with open('{}_facebook_comments.csv'.format(file_id), 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"message\"])\n",
    "\n",
    "        num_processed = 0\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "        after = ''\n",
    "        base = \"https://graph.facebook.com/v2.12\"\n",
    "        parameters = \"/?limit={}&access_token={}\".format(\n",
    "            100, access_token)\n",
    "\n",
    "        print(\"Scraping {} Comments From Posts: {}\\n\".format(\n",
    "            file_id, scrape_starttime))\n",
    "\n",
    "        with open('{}_facebook_statuses.csv'.format(file_id), 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "\n",
    "            # Uncomment below line to scrape comments for a specific status_id\n",
    "            # reader = [dict(status_id='5550296508_10154352768246509')]\n",
    "\n",
    "            for status in reader:\n",
    "                has_next_page = True\n",
    "\n",
    "                while has_next_page:\n",
    "\n",
    "                    node = \"/{}/comments\".format(status['status_id'])\n",
    "                    after = '' if after is '' else \"&after={}\".format(after)\n",
    "                    base_url = base + node + parameters + after\n",
    "\n",
    "                    # DEBUG: BASE URL\n",
    "                    # print(base_url)\n",
    "\n",
    "                    url = getFacebookCommentFeedUrl(base_url)\n",
    "                    \n",
    "                    # DEBUG: URL\n",
    "                    # print(url)\n",
    "                    \n",
    "                    reqResult = request_until_succeed(url)\n",
    "\n",
    "                    if(reqResult != 0):\n",
    "                        comments = json.loads(reqResult)\n",
    "\n",
    "                    for comment in comments['data']:\n",
    "                        comment_data = processFacebookComment(\n",
    "                            comment, status['status_id'])\n",
    "\n",
    "                        # calculate thankful/pride through algebra\n",
    "                        if(len(comment_data[0]) > 0):\n",
    "                            w.writerow(comment_data)\n",
    "\n",
    "                        if 'comments' in comment:\n",
    "                            has_next_subpage = True\n",
    "                            sub_after = ''\n",
    "\n",
    "                            while has_next_subpage:\n",
    "                                sub_node = \"/{}/comments\".format(comment['id'])\n",
    "                                sub_after = '' if sub_after is '' else \"&after={}\".format(\n",
    "                                    sub_after)\n",
    "                                sub_base_url = base + sub_node + parameters + sub_after\n",
    "\n",
    "                                sub_url = getFacebookCommentFeedUrl(\n",
    "                                    sub_base_url)\n",
    "\n",
    "                                reqResult = request_until_succeed(sub_url)\n",
    "                                \n",
    "                                if(reqResult != 0):\n",
    "                                    sub_comments = json.loads(reqResult)\n",
    "                                \n",
    "\n",
    "                                for sub_comment in sub_comments['data']:\n",
    "                                    sub_comment_data = processFacebookComment(\n",
    "                                        sub_comment, status['status_id'], comment['id'])\n",
    "\n",
    "                                    if(len(sub_comment_data[0]) >0):\n",
    "                                        w.writerow(sub_comment_data)\n",
    "\n",
    "                                    num_processed += 1\n",
    "                                    if num_processed % 100 == 0:\n",
    "                                        print(\"{} Comments Processed: {}\".format(\n",
    "                                            num_processed,\n",
    "                                            datetime.datetime.now()))\n",
    "\n",
    "                                if 'paging' in sub_comments:\n",
    "                                    if 'next' in sub_comments['paging']:\n",
    "                                        sub_after = sub_comments[\n",
    "                                            'paging']['cursors']['after']\n",
    "                                    else:\n",
    "                                        has_next_subpage = False\n",
    "                                else:\n",
    "                                    has_next_subpage = False\n",
    "\n",
    "                        # output progress occasionally to make sure code is not\n",
    "                        # stalling\n",
    "                        num_processed += 1\n",
    "                        if num_processed % 1000 == 0:\n",
    "                            print(\"{} Comments from facebook Processed: {}\".format(\n",
    "                                num_processed, datetime.datetime.now()))\n",
    "                        if num_processed >= 2000:\n",
    "                            break\n",
    "                    if 'paging' in comments:\n",
    "                        if 'next' in comments['paging']:\n",
    "                            after = comments['paging']['cursors']['after']\n",
    "                        else:\n",
    "                            has_next_page = False\n",
    "                    else:\n",
    "                        has_next_page = False\n",
    "\n",
    "        print(\"\\nDone!\\n\")\n",
    "\n",
    "def facebookscraper(page_id):\n",
    "    \n",
    "    app_id = \"155822221799033\"\n",
    "    app_secret = \"9f827ca98de88187071454a5fcceaa80\"\n",
    "    page_id = \"arsenal\"\n",
    "    file_id = page_id                           # set file_id same as page_id\n",
    "\n",
    "    # input date formatted as YYYY-MM-DD\n",
    "    since_date = \"2017-12-01\"\n",
    "    until_date = \"\"\n",
    "\n",
    "    access_token = app_id + \"|\" + app_secret\n",
    "    scrapeFacebookPageFeedStatus(page_id, access_token, since_date, until_date)\n",
    "    scrapeFacebookPageFeedComments(file_id, access_token,file_id)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "class Normalizer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def normalize(self, text_input):\n",
    "        text = text_input.split(' ')\n",
    "        text = [x.lower() for x in text]\n",
    "        text = [x.replace(\"\\\\n\",\" \") for x in text ]        \n",
    "        text = [x.replace(\"\\\\t\",\" \") for x in text ]        \n",
    "        text = [x.replace(\"\\\\xa0\",\" \") for x in text ]\n",
    "        text = [x.replace(\"\\\\xc2\",\" \") for x in text ]\n",
    "\n",
    "        #text = [x.replace(\",\",\" \").replace(\".\",\" \").replace(\" \", \"  \") for x in text ]\n",
    "        #text = [re.subn(\" ([a-z]) \",\"\\\\1\", x)[0] for x in text ]  \n",
    "        #text = [x.replace(\"  \",\" \") for x in text ]\n",
    "\n",
    "        text = [x.replace(\" u \",\" you \") for x in text ]\n",
    "        text = [x.replace(\" em \",\" them \") for x in text ]\n",
    "        text = [x.replace(\" da \",\" the \") for x in text ]\n",
    "        text = [x.replace(\" yo \",\" you \") for x in text ]\n",
    "        text = [x.replace(\" ur \",\" you \") for x in text ]\n",
    "        #text = [x.replace(\" ur \",\" your \") for x in text ]\n",
    "        #text = [x.replace(\" ur \",\" you're \") for x in text ]\n",
    "\n",
    "        text = [x.replace(\"won't\", \"will not\") for x in text ]\n",
    "        text = [x.replace(\"can't\", \"cannot\") for x in text ]\n",
    "        text = [x.replace(\"i'm\", \"i am\") for x in text ]\n",
    "        text = [x.replace(\" im \", \" i am \") for x in text ]\n",
    "        text = [x.replace(\"ain't\", \"is not\") for x in text ]\n",
    "        text = [x.replace(\"'ll\", \" will\") for x in text ]\n",
    "        text = [x.replace(\"'t\", \" not\") for x in text ]\n",
    "        text = [x.replace(\"'ve\", \" have\") for x in text ]\n",
    "        text = [x.replace(\"'s\", \" is\") for x in text ]\n",
    "        text = [x.replace(\"'re\", \" are\") for x in text ]\n",
    "        text = [x.replace(\"'d\", \" would\") for x in text ]\n",
    "        \n",
    "        punctuation = set(string.punctuation)\n",
    "        text_new = []\n",
    "        for word in text:\n",
    "            word = ''.join([c for c in word.lower() if not c in punctuation])\n",
    "            text_new.append(word);\n",
    "        string_new = \" \".join(text_new)\n",
    "        return string_new\n",
    "    \n",
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "class POSTagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]        \n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])\n",
    "\n",
    "def sentiment_anlysis(text):\n",
    "    \n",
    "    normalizer = Normalizer()    \n",
    "    splitter = Splitter()\n",
    "    postagger = POSTagger()\n",
    "    dicttagger = DictionaryTagger([ 'dicts/positive.yml', 'dicts/negative.yml', \n",
    "                                    'dicts/inc.yml', 'dicts/dec.yml', 'dicts/inv.yml'])\n",
    "\n",
    "    normalized_text = normalizer.normalize(text)\n",
    "    \n",
    "    splitted_sentences = splitter.split(normalized_text)\n",
    "    \n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    length = 1\n",
    "    if score != 0:\n",
    "        lengthlist = []\n",
    "        for n in splitted_sentences:\n",
    "            lengthlist = lengthlist + n\n",
    "        length = len(lengthlist)    \n",
    "    return (score / (length))\n",
    "\n",
    "\n",
    "def plot_result(x):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(x)\n",
    "    plt.title('Histogram of score distribution')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "import csv\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentimentResult(inputfilename):\n",
    "    filename = inputfilename.split('.')\n",
    "    outputfile1 = filename[0] + '_overall_score.csv'  \n",
    "    outputfile2 = filename[0] + '_all_score.csv'\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    with open(outputfile1, 'wb') as csvfile:\n",
    "        spamwriter1 = csv.DictWriter(csvfile,fieldnames=['score'])\n",
    "        spamwriter1.writeheader()\n",
    "        with open(outputfile2, 'wb') as csvfile:\n",
    "            spamwriter = csv.DictWriter(csvfile,fieldnames=['Message', 'Abusive_Score', 'NLTK_Neg', 'NLTK_Neu','NLTK_Pos', 'NLTK_Compound', 'Overall_score'])\n",
    "            spamwriter.writeheader()\n",
    "            score_sheet = []\n",
    "            with open(inputfilename, 'rb') as csvfile:\n",
    "                print 'Start doing sentiments analysis with file',inputfilename\n",
    "                spamreader = csv.reader(csvfile, delimiter='\\t')\n",
    "                i = 0;\n",
    "                for row in spamreader:\n",
    "                    i += 1\n",
    "                    ## stack overflow for recursion\n",
    "                    try:\n",
    "                        line = unicode(row[0][0:min(900,len(row[0]))], 'utf-8').lower()                    \n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    score = sentiment_anlysis(line)\n",
    "\n",
    "                    if i % 500 == 0: \n",
    "                        print 'sentiment analysis done at index', i  \n",
    "                    result = analyzer.polarity_scores(line)\n",
    "                    #print(result)\n",
    "                    if(score != 0):\n",
    "                        overall_score = max(min(score/(score*score+15)**(1/2.0)*10,-1*result['neg']),-1)\n",
    "                    else:\n",
    "                        overall_score = result['compound']\n",
    "                    score_sheet.append(overall_score)\n",
    "                    spamwriter1.writerow({'score':overall_score})\n",
    "                    my_result = {'Message': row[0], 'Abusive_Score': score, 'NLTK_Neg': result['neg'], 'NLTK_Neu': result['neu'], 'NLTK_Pos': result['pos'], 'NLTK_Compound': result['compound'],'Overall_score' : overall_score}\n",
    "                    if(score !=0):\n",
    "                        spamwriter.writerow(my_result)\n",
    "    return score_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot(x, titleName):\n",
    "    plt.xlim([-1, 1])\n",
    "    #bins = np.arange(-1, 1, 20)\n",
    "    #np.histogram(x, bins=20)\n",
    "    plt.hist(x, bins = 20)\n",
    "    plt.title(titleName)\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Number of Comments\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_pie(sizes, titleName):\n",
    "    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "    labels = 'Negative -1 to -0.5', 'Negative -0.5 to 0', 'Neutral', 'Positive 0 to +0.5', 'Positive +0.5 to +1'\n",
    "    explode = (0.1, 0.1, 0, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "    \n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=False, startangle=90)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title(titleName)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mycsv_reader(csv_reader):\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(csv_reader)\n",
    "        except csv.Error:\n",
    "            # error handling what you want.\n",
    "            pass\n",
    "        continue\n",
    "\n",
    "    return\n",
    "\n",
    "def myPlot(dirname, titleName):\n",
    "    fileDir = dirname\n",
    "    fileList = []\n",
    "    score_sheet = []\n",
    "    \n",
    "    negative10 = 0\n",
    "    negative05 = 0\n",
    "    neutral = 0\n",
    "    positive05 =0\n",
    "    positive10 = 0\n",
    "    \n",
    "    # read all filename in the directory and store in fileList as List\n",
    "    for file in os.listdir(fileDir):\n",
    "        name = fileDir + '/' + file\n",
    "        fileList.append(name)\n",
    "    \n",
    "    # read all file in the fileList and append score to score_sheet\n",
    "    for filename in fileList:\n",
    "        i = 0;\n",
    "        with open (filename, 'rb') as csvfile:\n",
    "            spamreader = mycsv_reader(csv.reader(csvfile, delimiter='\\t'))\n",
    "            print(filename)\n",
    "            for row in spamreader:\n",
    "                if i == 0:\n",
    "                    i = 1;\n",
    "                else:\n",
    "                    overall_score = float(row[0])\n",
    "                    #print(overall_score)\n",
    "                    score_sheet.append(overall_score)\n",
    "                    \n",
    "                    # classify score\n",
    "                    if overall_score < -0.5:\n",
    "                        negative10 += 1\n",
    "                    \n",
    "                    elif overall_score < 0:\n",
    "                        negative05 += 1\n",
    "                    \n",
    "                    elif overall_score == 0:\n",
    "                        neutral += 1\n",
    "                    \n",
    "                    elif overall_score < 0.5:\n",
    "                        positive05 += 1\n",
    "                    \n",
    "                    else:\n",
    "                        positive10 += 1\n",
    "\n",
    "    pieList = [negative10, negative05, neutral, positive05, positive10]\n",
    "    plot_pie(pieList, titleName)\n",
    "\n",
    "    #print(type(score_sheet[0]))\n",
    "    #print(\"Data Volumn: \", len(score_sheet))\n",
    "    np.array(score_sheet).astype(np.float)\n",
    "    plot_hist(score_sheet, titleName)\n",
    "    # return score_sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    facebookscraper('arsenal')\n",
    "    print('2000 facebook Comments for arsenal')\n",
    "    redditScraper('gunners')\n",
    "    print('2000 Reddit Comments for arsenal')\n",
    "    facebook_comment_filename = 'arsenal_facebook_comments.csv'\n",
    "    reddit_comment_filename ='arsenal_reddit_comments.csv'\n",
    "    \n",
    "    sentimentResult('arsenal_facebook_comments.csv')\n",
    "    directory_facebook = 'facebook'\n",
    "    if not os.path.exists(directory_facebook):\n",
    "        os.makedirs(directory_facebook)\n",
    "    os.rename(\"arsenal_facebook_comments_overall_score.csv\", \"./facebook\")\n",
    "    print('arsenal facebook comments analysis finished')\n",
    "    \n",
    "    sentimentResult('arsenal_reddit_comments.csv')\n",
    "    directory_reddit = 'reddit'\n",
    "    if not os.path.exists(directory_reddit):\n",
    "        os.makedirs(directory_reddit)\n",
    "    os.rename(\"arsenal_reddit_comments_overall_score.csv\", \"./reddit\")\n",
    "    print('arsenal reddit comments analysis finished')\n",
    "    \n",
    "    myPlot('./facebook', 'Arsenal_demo_facebook')\n",
    "    myPlot('./reddit', 'Arsenal_demo_reddit')\n",
    "    \n",
    "    os.rename(\"arsenal_reddit_comments_all_score.csv\", \"./reddit\")\n",
    "    os.rename(\"arsenal_facebook_comments_all_score.csv\", \"./facebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
